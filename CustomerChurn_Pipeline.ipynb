{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "825ab7cf",
   "metadata": {},
   "source": [
    "Task 2: End-to-End ML Pipeline with Scikit-learn\n",
    "\n",
    " Objective\n",
    "Build a reusable and production-ready machine learning pipeline to predict customer churn using the Telco Churn Dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af689fab",
   "metadata": {},
   "source": [
    "STEP:-\n",
    "1. Load Dataset\n",
    "- Import the Telco Churn data.\n",
    "- Separate features (`X`) and target (`y`).\n",
    "- Encode the target variable (Yes → 1, No → 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4834d1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\admin\\downloads\\dev_hub_internship_advance\\env\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\downloads\\dev_hub_internship_advance\\env\\lib\\site-packages (from pandas) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\downloads\\dev_hub_internship_advance\\env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\admin\\downloads\\dev_hub_internship_advance\\env\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\downloads\\dev_hub_internship_advance\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc39a89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "   LoyaltyID Customer ID Senior Citizen Partner Dependents  Tenure  \\\n",
      "0     318537  7590-VHVEG             No     Yes         No       1   \n",
      "1     152148  5575-GNVDE             No      No         No      34   \n",
      "2     326527  3668-QPYBK             No      No         No       2   \n",
      "3     845894  7795-CFOCW             No      No         No      45   \n",
      "4     503388  9237-HQITU             No      No         No       2   \n",
      "\n",
      "  Phone Service    Multiple Lines Internet Service Online Security  \\\n",
      "0            No  No phone service              DSL              No   \n",
      "1           Yes                No              DSL             Yes   \n",
      "2           Yes                No              DSL             Yes   \n",
      "3            No  No phone service              DSL             Yes   \n",
      "4           Yes                No      Fiber optic              No   \n",
      "\n",
      "  Online Backup Device Protection Tech Support Streaming TV Streaming Movies  \\\n",
      "0           Yes                No           No           No               No   \n",
      "1            No               Yes           No           No               No   \n",
      "2           Yes                No           No           No               No   \n",
      "3            No               Yes          Yes           No               No   \n",
      "4            No                No           No           No               No   \n",
      "\n",
      "         Contract Paperless Billing             Payment Method  \\\n",
      "0  Month-to-month               Yes           Electronic check   \n",
      "1        One year                No               Mailed check   \n",
      "2  Month-to-month               Yes               Mailed check   \n",
      "3        One year                No  Bank transfer (automatic)   \n",
      "4  Month-to-month               Yes           Electronic check   \n",
      "\n",
      "   Monthly Charges Total Charges  \n",
      "0            29.85         29.85  \n",
      "1            56.95        1889.5  \n",
      "2            53.85        108.15  \n",
      "3            42.30       1840.75  \n",
      "4            70.70        151.65  \n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: Churn, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset from the same folder\n",
    "data = pd.read_csv(\"Telco-Customer-Churn.csv\")\n",
    "\n",
    "# Existing preprocessing\n",
    "X = data.drop(\"Churn\", axis=1)\n",
    "y = data[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(X.head())\n",
    "print(y.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f016873",
   "metadata": {},
   "source": [
    "STEP:-\n",
    "2. Identify Features\n",
    "- Separate **numeric** and categorical features for preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb35063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13448\\3326557856.py:2: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.\n",
      "See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.\n",
      "  categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n"
     ]
    }
   ],
   "source": [
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07569219",
   "metadata": {},
   "source": [
    "STEP:-\n",
    "3. Preprocessing Pipelines\n",
    "- Scale numeric features using `StandardScaler`.\n",
    "- Encode categorical features using `OneHotEncoder`.\n",
    "- Combine both using `ColumnTransformer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d97cc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec307cf",
   "metadata": {},
   "source": [
    "STEP:-\n",
    "4. Build Full Pipelines\n",
    "- Create pipelines for **Logistic Regression** and **Random Forest**.\n",
    "- Include preprocessing and model training in the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485ce50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Logistic Regression pipeline\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "# Random Forest pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6f424",
   "metadata": {},
   "source": [
    " STEP:-\n",
    " 5. Hyperparameter Tuning\n",
    "- Use `GridSearchCV` to find the best hyperparameters for Random Forest.\n",
    "- Use cross-validation to evaluate performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4adc1018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'classifier__max_depth': None, 'classifier__n_estimators': 200}\n",
      "Best score: 0.7928433890896186\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [5, 10, None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf_pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce361c9",
   "metadata": {},
   "source": [
    "STEP:-\n",
    "6. Export Pipeline\n",
    "- Save the trained pipeline using `joblib` for production reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa05e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline saved as 'telco_churn_pipeline.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(grid_search.best_estimator_, \"telco_churn_pipeline.pkl\")\n",
    "print(\"Pipeline saved as 'telco_churn_pipeline.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e56b1",
   "metadata": {},
   "source": [
    "STEP:-\n",
    "7. Load and Use Saved Pipeline\n",
    "- Load the saved pipeline for making predictions on new customer data.\n",
    "- Output:\n",
    "  - `prediction` → 0 (No churn) / 1 (Churn)\n",
    "  - `prediction_proba` → probability of churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a77c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: ['LoyaltyID', 'Customer ID', 'Senior Citizen', 'Partner', 'Dependents', 'Tenure', 'Phone Service', 'Multiple Lines', 'Internet Service', 'Online Security', 'Online Backup', 'Device Protection', 'Tech Support', 'Streaming TV', 'Streaming Movies', 'Contract', 'Paperless Billing', 'Payment Method', 'Monthly Charges', 'Total Charges']\n",
      "Warning: Column 'tenure' not found in dataset!\n",
      "Warning: Column 'MonthlyCharges' not found in dataset!\n",
      "Warning: Column 'TotalCharges' not found in dataset!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3641\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3642\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:168\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:197\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7668\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7676\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'LoyaltyID'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:469\u001b[39m, in \u001b[36m_get_column_indices\u001b[39m\u001b[34m(X, key)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     col_idx = \u001b[43mall_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers.Integral):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3648\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3651\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3652\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'LoyaltyID'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     42\u001b[39m pipeline = Pipeline([\n\u001b[32m     43\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocessor\u001b[39m\u001b[33m'\u001b[39m, preprocessor),\n\u001b[32m     44\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m'\u001b[39m, RandomForestClassifier(random_state=\u001b[32m42\u001b[39m))\n\u001b[32m     45\u001b[39m ])\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Fit pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# New customer data (column names must match CSV exactly)\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Ensure keys match the CSV header exactly\u001b[39;00m\n\u001b[32m     52\u001b[39m new_data = pd.DataFrame([{\n\u001b[32m     53\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mgender\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mFemale\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     54\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mSenior Citizen\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[38;5;66;03m# Check if 'Senior Citizen' or 'SeniorCitizen' in CSV\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mTotal Charges\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m876.5\u001b[39m\n\u001b[32m     72\u001b[39m }])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\sklearn\\pipeline.py:613\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    606\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    607\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    608\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m     )\n\u001b[32m    612\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    615\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\sklearn\\pipeline.py:547\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    541\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    542\u001b[39m     step_idx=step_idx,\n\u001b[32m    543\u001b[39m     step_params=routed_params[name],\n\u001b[32m    544\u001b[39m     all_params=raw_params,\n\u001b[32m    545\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\joblib\\memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\sklearn\\pipeline.py:1484\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1482\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1483\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1486\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1487\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1488\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:991\u001b[39m, in \u001b[36mColumnTransformer.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    988\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_transformers()\n\u001b[32m    989\u001b[39m n_samples = _num_samples(X)\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_remainder(X)\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:545\u001b[39m, in \u001b[36mColumnTransformer._validate_column_callables\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    543\u001b[39m         columns = columns(X)\n\u001b[32m    544\u001b[39m     all_columns.append(columns)\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m     transformer_to_input_indices[name] = \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28mself\u001b[39m._columns = all_columns\n\u001b[32m    548\u001b[39m \u001b[38;5;28mself\u001b[39m._transformer_to_input_indices = transformer_to_input_indices\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Downloads\\DEV_HUB_Internship_Advance\\env\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:477\u001b[39m, in \u001b[36m_get_column_indices\u001b[39m\u001b[34m(X, key)\u001b[39m\n\u001b[32m    474\u001b[39m         column_indices.append(col_idx)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mA given column is not a column of the dataframe\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n",
      "\u001b[31mValueError\u001b[39m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. Load data\n",
    "data = pd.read_csv(\"Telco-Customer-Churn.csv\")\n",
    "\n",
    "# 2. Setup Target (y)\n",
    "# Standard Telco datasets use 'Churn'\n",
    "y = data[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "# 3. Setup Features (X)\n",
    "# Drop the Target and IDs. I added 'errors=ignore' so it won't crash if a column is missing.\n",
    "X = data.drop(columns=['Churn', 'LoyaltyID', 'Customer ID'], errors='ignore')\n",
    "\n",
    "# 4. Define Numeric Features (MATCHING YOUR PRINTED LIST EXACTLY)\n",
    "numeric_features = ['Tenure', 'Monthly Charges', 'Total Charges']\n",
    "\n",
    "for col in numeric_features:\n",
    "    if col in X.columns:\n",
    "        # Convert to string first to strip spaces, then to numeric\n",
    "        X[col] = pd.to_numeric(X[col].astype(str).str.strip(), errors='coerce')\n",
    "\n",
    "# Drop rows with NaN in numeric columns (common in Total Charges)\n",
    "X = X.dropna(subset=numeric_features)\n",
    "y = y[X.index] # Keep y aligned with X\n",
    "\n",
    "# 5. Define Categorical Features\n",
    "categorical_features = [col for col in X.columns if col not in numeric_features]\n",
    "\n",
    "# 6. Build the Pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# 7. Fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# 8. Create New Customer Data (NAMES MUST MATCH X COLUMNS EXACTLY)\n",
    "new_data = pd.DataFrame([{\n",
    "    'Senior Citizen': 0,\n",
    "    'Partner': 'Yes',\n",
    "    'Dependents': 'No',\n",
    "    'Tenure': 12,\n",
    "    'Phone Service': 'Yes',\n",
    "    'Multiple Lines': 'No',\n",
    "    'Internet Service': 'Fiber optic',\n",
    "    'Online Security': 'No',\n",
    "    'Online Backup': 'Yes',\n",
    "    'Device Protection': 'No',\n",
    "    'Tech Support': 'No',\n",
    "    'Streaming TV': 'Yes',\n",
    "    'Streaming Movies': 'No',\n",
    "    'Contract': 'Month-to-month',\n",
    "    'Paperless Billing': 'Yes',\n",
    "    'Payment Method': 'Electronic check',\n",
    "    'Monthly Charges': 85.5,\n",
    "    'Total Charges': 876.5,\n",
    "    'gender': 'Female' # Added this to match typical dataset\n",
    "}])\n",
    "\n",
    "# Ensure new_data has all columns in the same order/style as training X\n",
    "# We filter new_data to only include columns the pipeline was trained on\n",
    "new_data_filtered = new_data[X.columns]\n",
    "\n",
    "# 9. Predict\n",
    "prediction = pipeline.predict(new_data_filtered)\n",
    "prediction_proba = pipeline.predict_proba(new_data_filtered)\n",
    "\n",
    "print(\"--- Result ---\")\n",
    "print(\"Predicted Churn (0=No, 1=Yes):\", prediction[0])\n",
    "print(\"Prediction probability:\", prediction_proba[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
